{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import json #storing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_data = pd.read_csv(\"../sample_data/politics_sample3.csv\", \n",
    "                            header=None, \n",
    "                            names = ['author', 'subreddit', 'body', 'score', 'created_dt', 'link_id', 'parent_id'],\n",
    "                            index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 2, 24, 0, 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.fromisoformat(politics_data['created_dt'][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_text = politics_data.loc[politics_data['body'].notna()]['body'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_dates = politics_data.loc[politics_data['body'].notna()]['created_dt'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_date_objs = [datetime.fromisoformat(i[:10]) for i in politics_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime(2015, 2, 1) < datetime(2015, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999960\n",
      "999960\n"
     ]
    }
   ],
   "source": [
    "print(len(politics_text))\n",
    "print(len(politics_date_objs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20753\n",
      "39586\n",
      "122760\n",
      "247798\n",
      "999960\n"
     ]
    }
   ],
   "source": [
    "print(len([i for i in politics_date_objs if i < datetime(2015, 2, 1)]))\n",
    "print(len([i for i in politics_date_objs if i < datetime(2015, 3, 1)]))\n",
    "print(len([i for i in politics_date_objs if i >= datetime(2016, 5, 1)]))\n",
    "print(len([i for i in politics_date_objs if i >= datetime(2016, 4, 1)]))\n",
    "print(len([i for i in politics_date_objs if i < datetime(2016, 6, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [datetime(2015, i, 1) for i in range(1,13)] + [datetime(2016, i, 1) for i in range(1,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_indices = []\n",
    "\n",
    "for i in range(len(months) - 1):\n",
    "    month_indices.append([j for j in range(len(politics_date_objs)) \n",
    "                         if politics_date_objs[j] >= months[i]  \n",
    "                         and politics_date_objs[j] < months[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 17 artists>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(months)-1), [len(i) for i in month_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_by_month = []\n",
    "\n",
    "for lst in month_indices:\n",
    "    text_by_month.append([politics_text[i] for i in lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    print(\"Number of entries in original dataset: \" + str(len(data)))\n",
    "    data1 = [i for i in data if 'thank you for participating' not in i.lower()]\n",
    "    print(\"Number of moderator posts removed from dataset: \" + str(len([i for i in data if 'thank you for participating' in i.lower()])))\n",
    "    data1 = [i for i in data1 if i != '[deleted]' and i != '[removed]']\n",
    "    print(\"Number of user-deleted posts removed from dataset: \" + str(len([i for i in data if i == '[deleted]'])))\n",
    "    print(\"Number of mod/admin-deleted posts removed from dataset: \" + str(len([i for i in data if i == '[removed]'])))\n",
    "    \n",
    "    print(\"Number of entries in modified dataset: \" + str(len(data1)))\n",
    "    \n",
    "    # removing hyperlinks\n",
    "    print(\"Number of comments with a hyperlink in modified dataset: \" + str(len([i for i in data1 if \"http\" in i])))\n",
    "    data1 = [re.sub(r'https?:\\/\\/[\\S]+', ' ', i, flags=re.MULTILINE) for i in data1]\n",
    "    \n",
    "    # common escape sequences\n",
    "    data1 = [re.sub(r'\\n', '', i) for i in data1]\n",
    "    data1 = [re.sub(r'&gt;', '', i) for i in data1]\n",
    "    \n",
    "    # convert to lowercase\n",
    "    data1 = [i.lower() for i in data1]\n",
    "    \n",
    "    # keep user and subreddit tags\n",
    "    print(\"Number of comments that mention a user in the modified dataset: \" + str(len([i for i in data1 if \"/u/\" in i])))\n",
    "    print(\"Number of comments that mention a subreddit in the modified dataset: \" + str(len([i for i in data1 if \"/r/\" in i])))\n",
    "    data1 = [re.sub(r'/u/', '_user_', i) for i in data1]\n",
    "    data1 = [re.sub(r'/r/', '_subreddit_', i) for i in data1]\n",
    "    \n",
    "    data_tokenized = [re.findall(r'\\w+', i.lower()) for i in data1]\n",
    "    \n",
    "    # remove numbers\n",
    "    data_tokenized = [[token for token in doc if not token.isnumeric()] for doc in data_tokenized]\n",
    "    \n",
    "    print(\"Number of total tokens: \" + str(sum([len(d) for d in data_tokenized])))\n",
    "    \n",
    "    return data_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in original dataset: 20753\n",
      "Number of moderator posts removed from dataset: 159\n",
      "Number of user-deleted posts removed from dataset: 1149\n",
      "Number of mod/admin-deleted posts removed from dataset: 0\n",
      "Number of entries in modified dataset: 19445\n",
      "Number of comments with a hyperlink in modified dataset: 1132\n",
      "Number of comments that mention a user in the modified dataset: 37\n",
      "Number of comments that mention a subreddit in the modified dataset: 141\n",
      "Number of total tokens: 887728\n",
      "\n",
      "Number of entries in original dataset: 18833\n",
      "Number of moderator posts removed from dataset: 155\n",
      "Number of user-deleted posts removed from dataset: 979\n",
      "Number of mod/admin-deleted posts removed from dataset: 0\n",
      "Number of entries in modified dataset: 17699\n",
      "Number of comments with a hyperlink in modified dataset: 1051\n",
      "Number of comments that mention a user in the modified dataset: 48\n",
      "Number of comments that mention a subreddit in the modified dataset: 116\n",
      "Number of total tokens: 808986\n",
      "\n",
      "Number of entries in original dataset: 23213\n",
      "Number of moderator posts removed from dataset: 162\n",
      "Number of user-deleted posts removed from dataset: 1293\n",
      "Number of mod/admin-deleted posts removed from dataset: 0\n",
      "Number of entries in modified dataset: 21758\n",
      "Number of comments with a hyperlink in modified dataset: 1311\n",
      "Number of comments that mention a user in the modified dataset: 56\n",
      "Number of comments that mention a subreddit in the modified dataset: 149\n",
      "Number of total tokens: 973409\n",
      "\n",
      "Number of entries in original dataset: 23882\n",
      "Number of moderator posts removed from dataset: 177\n",
      "Number of user-deleted posts removed from dataset: 1348\n",
      "Number of mod/admin-deleted posts removed from dataset: 0\n",
      "Number of entries in modified dataset: 22357\n",
      "Number of comments with a hyperlink in modified dataset: 1276\n",
      "Number of comments that mention a user in the modified dataset: 60\n",
      "Number of comments that mention a subreddit in the modified dataset: 170\n",
      "Number of total tokens: 1033178\n",
      "\n",
      "Number of entries in original dataset: 23345\n",
      "Number of moderator posts removed from dataset: 231\n",
      "Number of user-deleted posts removed from dataset: 1270\n",
      "Number of mod/admin-deleted posts removed from dataset: 0\n",
      "Number of entries in modified dataset: 21844\n",
      "Number of comments with a hyperlink in modified dataset: 1279\n",
      "Number of comments that mention a user in the modified dataset: 50\n",
      "Number of comments that mention a subreddit in the modified dataset: 194\n",
      "Number of total tokens: 999923\n",
      "\n",
      "Number of entries in original dataset: 26858\n",
      "Number of moderator posts removed from dataset: 286\n",
      "Number of user-deleted posts removed from dataset: 1570\n",
      "Number of mod/admin-deleted posts removed from dataset: 0\n",
      "Number of entries in modified dataset: 25002\n",
      "Number of comments with a hyperlink in modified dataset: 1497\n",
      "Number of comments that mention a user in the modified dataset: 72\n",
      "Number of comments that mention a subreddit in the modified dataset: 208\n",
      "Number of total tokens: 1154885\n",
      "\n",
      "Number of entries in original dataset: 32116\n",
      "Number of moderator posts removed from dataset: 341\n",
      "Number of user-deleted posts removed from dataset: 1634\n",
      "Number of mod/admin-deleted posts removed from dataset: 0\n",
      "Number of entries in modified dataset: 30141\n",
      "Number of comments with a hyperlink in modified dataset: 1781\n",
      "Number of comments that mention a user in the modified dataset: 90\n",
      "Number of comments that mention a subreddit in the modified dataset: 213\n",
      "Number of total tokens: 1392733\n",
      "\n",
      "Number of entries in original dataset: 36358\n",
      "Number of moderator posts removed from dataset: 345\n",
      "Number of user-deleted posts removed from dataset: 2077\n",
      "Number of mod/admin-deleted posts removed from dataset: 4\n",
      "Number of entries in modified dataset: 33932\n",
      "Number of comments with a hyperlink in modified dataset: 1997\n",
      "Number of comments that mention a user in the modified dataset: 66\n",
      "Number of comments that mention a subreddit in the modified dataset: 231\n",
      "Number of total tokens: 1506243\n",
      "\n",
      "Number of entries in original dataset: 39039\n",
      "Number of moderator posts removed from dataset: 379\n",
      "Number of user-deleted posts removed from dataset: 1654\n",
      "Number of mod/admin-deleted posts removed from dataset: 428\n",
      "Number of entries in modified dataset: 36578\n",
      "Number of comments with a hyperlink in modified dataset: 2055\n",
      "Number of comments that mention a user in the modified dataset: 83\n",
      "Number of comments that mention a subreddit in the modified dataset: 258\n",
      "Number of total tokens: 1566504\n",
      "\n",
      "Number of entries in original dataset: 50481\n",
      "Number of moderator posts removed from dataset: 387\n",
      "Number of user-deleted posts removed from dataset: 2185\n",
      "Number of mod/admin-deleted posts removed from dataset: 547\n",
      "Number of entries in modified dataset: 47362\n",
      "Number of comments with a hyperlink in modified dataset: 2841\n",
      "Number of comments that mention a user in the modified dataset: 102\n",
      "Number of comments that mention a subreddit in the modified dataset: 307\n",
      "Number of total tokens: 2019574\n",
      "\n",
      "Number of entries in original dataset: 48005\n",
      "Number of moderator posts removed from dataset: 486\n",
      "Number of user-deleted posts removed from dataset: 2204\n",
      "Number of mod/admin-deleted posts removed from dataset: 649\n",
      "Number of entries in modified dataset: 44666\n",
      "Number of comments with a hyperlink in modified dataset: 2737\n",
      "Number of comments that mention a user in the modified dataset: 98\n",
      "Number of comments that mention a subreddit in the modified dataset: 298\n",
      "Number of total tokens: 1929261\n",
      "\n",
      "Number of entries in original dataset: 56018\n",
      "Number of moderator posts removed from dataset: 566\n",
      "Number of user-deleted posts removed from dataset: 2658\n",
      "Number of mod/admin-deleted posts removed from dataset: 810\n",
      "Number of entries in modified dataset: 51984\n",
      "Number of comments with a hyperlink in modified dataset: 3076\n",
      "Number of comments that mention a user in the modified dataset: 89\n",
      "Number of comments that mention a subreddit in the modified dataset: 390\n",
      "Number of total tokens: 2214409\n",
      "\n",
      "Number of entries in original dataset: 73062\n",
      "Number of moderator posts removed from dataset: 694\n",
      "Number of user-deleted posts removed from dataset: 3487\n",
      "Number of mod/admin-deleted posts removed from dataset: 899\n",
      "Number of entries in modified dataset: 67982\n",
      "Number of comments with a hyperlink in modified dataset: 3947\n",
      "Number of comments that mention a user in the modified dataset: 145\n",
      "Number of comments that mention a subreddit in the modified dataset: 525\n",
      "Number of total tokens: 2748880\n",
      "\n",
      "Number of entries in original dataset: 125579\n",
      "Number of moderator posts removed from dataset: 1194\n",
      "Number of user-deleted posts removed from dataset: 7247\n",
      "Number of mod/admin-deleted posts removed from dataset: 1758\n",
      "Number of entries in modified dataset: 115380\n",
      "Number of comments with a hyperlink in modified dataset: 6072\n",
      "Number of comments that mention a user in the modified dataset: 229\n",
      "Number of comments that mention a subreddit in the modified dataset: 909\n",
      "Number of total tokens: 4309968\n",
      "\n",
      "Number of entries in original dataset: 154620\n",
      "Number of moderator posts removed from dataset: 1081\n",
      "Number of user-deleted posts removed from dataset: 8689\n",
      "Number of mod/admin-deleted posts removed from dataset: 2160\n",
      "Number of entries in modified dataset: 142690\n",
      "Number of comments with a hyperlink in modified dataset: 7602\n",
      "Number of comments that mention a user in the modified dataset: 248\n",
      "Number of comments that mention a subreddit in the modified dataset: 1300\n",
      "Number of total tokens: 5308034\n",
      "\n",
      "Number of entries in original dataset: 125038\n",
      "Number of moderator posts removed from dataset: 555\n",
      "Number of user-deleted posts removed from dataset: 7017\n",
      "Number of mod/admin-deleted posts removed from dataset: 2234\n",
      "Number of entries in modified dataset: 115232\n",
      "Number of comments with a hyperlink in modified dataset: 6464\n",
      "Number of comments that mention a user in the modified dataset: 282\n",
      "Number of comments that mention a subreddit in the modified dataset: 1216\n",
      "Number of total tokens: 4562512\n",
      "\n",
      "Number of entries in original dataset: 122760\n",
      "Number of moderator posts removed from dataset: 584\n",
      "Number of user-deleted posts removed from dataset: 6572\n",
      "Number of mod/admin-deleted posts removed from dataset: 3719\n",
      "Number of entries in modified dataset: 111885\n",
      "Number of comments with a hyperlink in modified dataset: 6236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments that mention a user in the modified dataset: 238\n",
      "Number of comments that mention a subreddit in the modified dataset: 1030\n",
      "Number of total tokens: 4422436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_by_month = []\n",
    "\n",
    "for lst in text_by_month:\n",
    "    tokens_by_month.append(tokenize_data(lst))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokens_by_month)):\n",
    "    with open(\"stored_variables/politics_tokens_month\" + str(i) + \".json\", 'w') as f:\n",
    "        json.dump(tokens_by_month[i], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_comments(chunk, month_vals):\n",
    "    chunk_notna = chunk.loc[chunk['body'].notna()]\n",
    "    comments = chunk_notna['body'].values\n",
    "    auths = chunk_notna['author'].values\n",
    "    date_vals = chunk_notna['created_dt'].values\n",
    "    date_objs = [datetime.fromisoformat(i[:10]) for i in date_vals]\n",
    "    \n",
    "    comment_lists = []\n",
    "    auth_lists = []\n",
    "    \n",
    "    for i in range(len(month_vals) - 1):\n",
    "        time_indices = [j for j in range(len(date_objs)) if date_objs[j] >= month_vals[i]\n",
    "                        and date_objs[j] < month_vals[i+1]]\n",
    "        \n",
    "        good_indices = [j for j in time_indices if comments[j] != '[deleted]' \n",
    "                        and comments[j] != '[removed]'\n",
    "                        and 'thank you for participating' not in comments[j].lower()]\n",
    "        \n",
    "        comment_lists.append(list(comments[good_indices]))\n",
    "        auth_lists.append(list(auths[good_indices]))\n",
    "        \n",
    "    return (comment_lists, auth_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_comments(data):\n",
    "    # remove hyperlinks\n",
    "    data1 = [re.sub(r'https?:\\/\\/[\\S]+', ' ', i, flags=re.MULTILINE) for i in data]\n",
    "    \n",
    "    # common escape sequences\n",
    "    data1 = [re.sub(r'\\n', '', i) for i in data1]\n",
    "    data1 = [re.sub(r'&gt;', '', i) for i in data1]\n",
    "    \n",
    "    # convert to lowercase\n",
    "    data1 = [i.lower() for i in data1]\n",
    "    \n",
    "    # keep user and subreddit tags\n",
    "    data1 = [re.sub(r'/u/', '_user_', i) for i in data1]\n",
    "    data1 = [re.sub(r'/r/', '_subreddit_', i) for i in data1]\n",
    "    \n",
    "    data_tokenized = [re.findall(r'\\w+', i.lower()) for i in data1]\n",
    "    \n",
    "    # remove numbers\n",
    "    data_tokenized = [[token for token in doc if not token.isnumeric()] for doc in data_tokenized]\n",
    "    \n",
    "    return data_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_auths = [[]] * 24\n",
    "comment_tokens = [[]] * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "months2 = [datetime(2015, i, 1) for i in range(1,13)] + [datetime(2016, i, 1) for i in range(1,13)] + \\\n",
    "[datetime(2017, 1, 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# test appending\n",
    "with open(\"test_file.json\", 'w') as f:\n",
    "    f.write('[')\n",
    "    \n",
    "with open(\"test_file.json\", 'a') as f:\n",
    "    f.write(json.dumps([1,2,3])[1:-1])\n",
    "    f.write(',')\n",
    "    \n",
    "with open('test_file.json', 'a') as f:\n",
    "    f.write(json.dumps([4,5,6])[1:-1])\n",
    "    f.write(']')\n",
    "    \n",
    "with open('test_file.json', 'r') as f:\n",
    "    print(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(24):\n",
    "    with open(\"stored_variables/politics_tokens2_month\" + str(i) + \".json\", 'w') as f:\n",
    "        f.write('[')\n",
    "        \n",
    "for i in range(len(comment_auths)):\n",
    "    with open(\"stored_variables/politics_auths2_month\" + str(i) + \".json\", 'w') as f:\n",
    "        f.write('[')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n",
      "2500000\n",
      "3000000\n",
      "3500000\n",
      "4000000\n",
      "4500000\n",
      "5000000\n",
      "5500000\n",
      "6000000\n",
      "6500000\n",
      "7000000\n",
      "7500000\n",
      "8000000\n",
      "8500000\n",
      "9000000\n",
      "9500000\n",
      "10000000\n",
      "10500000\n",
      "11000000\n",
      "11500000\n",
      "12000000\n",
      "12500000\n",
      "13000000\n",
      "13500000\n",
      "14000000\n",
      "14500000\n",
      "15000000\n",
      "15500000\n",
      "16000000\n",
      "16500000\n",
      "17000000\n",
      "17500000\n",
      "18000000\n",
      "18500000\n",
      "19000000\n",
      "19500000\n",
      "20000000\n",
      "20500000\n",
      "21000000\n",
      "21500000\n",
      "22000000\n",
      "22500000\n",
      "23000000\n",
      "23500000\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 5 * (10 ** 5)\n",
    "iter_num = 0\n",
    "not_first = [False] * 24\n",
    "\n",
    "for chunk in pd.read_csv(\"../sample_data/politics_comments_2015_2016_2.csv\", \n",
    "                         header=None, \n",
    "                         names = ['author', 'subreddit', 'body', 'score', 'created_dt', 'link_id', 'parent_id', 'id'],\n",
    "                         usecols = ['author','body', 'created_dt'],\n",
    "                         index_col = False,\n",
    "                         chunksize = chunk_size\n",
    "                         ):\n",
    "    iter_num += 1\n",
    "    comments, auths = remove_bad_comments(chunk, months2)\n",
    "    \n",
    "    for i in range(len(months2) - 1):\n",
    "        if len(auths[i]) > 0:\n",
    "            comment_toks = tokenize_comments(comments[i])\n",
    "\n",
    "            with open(\"stored_variables/politics_tokens2_month\" + str(i) + \".json\", 'a') as f:\n",
    "                if not_first[i]:\n",
    "                    f.write(',')\n",
    "                f.write(json.dumps(comment_toks)[1:-1])\n",
    "\n",
    "            with open(\"stored_variables/politics_auths2_month\" + str(i) + \".json\", 'a') as f:\n",
    "                if not_first[i]:\n",
    "                    f.write(',')\n",
    "                f.write(json.dumps(auths[i])[1:-1])\n",
    "\n",
    "            not_first[i] = True\n",
    "    \n",
    "    print(iter_num * chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(24):\n",
    "    with open(\"stored_variables/politics_tokens2_month\" + str(i) + \".json\", 'a') as f:\n",
    "        f.write(']')\n",
    "        \n",
    "for i in range(len(comment_auths)):\n",
    "    with open(\"stored_variables/politics_auths2_month\" + str(i) + \".json\", 'a') as f:\n",
    "        f.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
