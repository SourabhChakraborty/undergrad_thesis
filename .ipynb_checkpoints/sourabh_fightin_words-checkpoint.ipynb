{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stored_variables/sfp_subreddits_by_author.json', 'r') as f:\n",
    "    sfp_subreddits_by_author = json.load(f)\n",
    "\n",
    "with open('stored_variables/td_subreddits_by_author.json', 'r') as f:\n",
    "    td_subreddits_by_author = json.load(f)\n",
    "    \n",
    "with open('stored_variables/neither_subreddits_by_author.json', 'r') as f:\n",
    "    neither_subreddits_by_author = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: uniform prior, un-normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfp_subreddits_list = []\n",
    "\n",
    "for author in sfp_subreddits_by_author.keys():\n",
    "    curr_str = \"\"\n",
    "    \n",
    "    for subreddit in sfp_subreddits_by_author[author].keys():\n",
    "        curr_str += (subreddit + \" \")\n",
    "        \n",
    "    sfp_subreddits_list.append(curr_str)\n",
    "\n",
    "td_subreddits_list = []\n",
    "\n",
    "for author in td_subreddits_by_author.keys():\n",
    "    curr_str = \"\"\n",
    "    \n",
    "    for subreddit in td_subreddits_by_author[author].keys():\n",
    "        curr_str += subreddit + \" \"\n",
    "    \n",
    "    td_subreddits_list.append(curr_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare1(l1, l2, prior=0.01):\n",
    "    cv = CV(decode_error = 'ignore', min_df = 10, max_df = .5, ngram_range=(1,1),\n",
    "        binary = False,\n",
    "        max_features = 15000)\n",
    "    \n",
    "    counts_mat = cv.fit_transform(l1 + l2).toarray()\n",
    "    vocab_size = len(cv.vocabulary_)\n",
    "    #print(vocab_size)\n",
    "    priors = np.array([prior for i in range(vocab_size)])\n",
    "    \n",
    "    z_scores = np.empty(priors.shape[0])\n",
    "    count_matrix = np.empty([2, vocab_size], dtype=np.float32)\n",
    "    count_matrix[0, :] = np.sum(counts_mat[:len(l1), :], axis = 0)\n",
    "    count_matrix[1, :] = np.sum(counts_mat[len(l1):, :], axis = 0)\n",
    "    a0 = np.sum(priors)\n",
    "    n1 = 1.*np.sum(count_matrix[0,:])\n",
    "    n2 = 1.*np.sum(count_matrix[1,:])\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        #compute delta\n",
    "        term1 = np.log((count_matrix[0,i] + priors[i])/(n1 + a0 - count_matrix[0,i] - priors[i]))\n",
    "        term2 = np.log((count_matrix[1,i] + priors[i])/(n2 + a0 - count_matrix[1,i] - priors[i]))        \n",
    "        delta = term1 - term2\n",
    "        #compute variance on delta\n",
    "        var = 1./(count_matrix[0,i] + priors[i]) + 1./(count_matrix[1,i] + priors[i])\n",
    "        #store final score\n",
    "        z_scores[i] = delta/np.sqrt(var)\n",
    "    index_to_term = {v:k for k,v in cv.vocabulary_.items()}\n",
    "    sorted_indices = np.argsort(z_scores)\n",
    "    return_list = []\n",
    "    for i in sorted_indices:\n",
    "        return_list.append((index_to_term[i], z_scores[i]))\n",
    "        \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thebutton', 3.777473337176859), ('askscience', 3.97654623114658), ('progressive', 4.044684957814867), ('restorethefourth', 4.074991038357249), ('seattle', 4.126992184683207), ('secretsanta', 4.343393488401389), ('basicincome', 4.488484208518572), ('doctorwho', 4.499111229201192), ('occupywallstreet', 5.036168985066751), ('blog', 5.523181789875759)]\n",
      "[('pokemongo', -7.587610454482942), ('cringeanarchy', -7.480103708528793), ('overwatch', -7.409909049309773), ('globaloffensive', -6.2801295838618945), ('roastme', -6.062970625925755), ('4chan', -5.892696460418769), ('dota2', -4.887291774343714), ('thedivision', -4.858327105652409), ('theredpill', -4.789393729985014), ('2007scape', -4.7164541864877565)]\n"
     ]
    }
   ],
   "source": [
    "comparison_1 = compare1(sfp_subreddits_list, td_subreddits_list)\n",
    "print(comparison_1[-10:])\n",
    "print(comparison_1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare2(l1, l2, prior=0.01):\n",
    "    cv = CV(decode_error = 'ignore', min_df = 10, max_df = .5, ngram_range=(1,1),\n",
    "        binary = False,\n",
    "        max_features = 15000)\n",
    "    \n",
    "    counts_mat = cv.fit_transform(l1 + l2).toarray()\n",
    "    vocab_size = len(cv.vocabulary_)\n",
    "    #print(vocab_size)\n",
    "    priors = np.array([prior for i in range(vocab_size)])\n",
    "    \n",
    "    #normalization:\n",
    "    counts_mat = (counts_mat.T/(np.sum(counts_mat, axis=1) + 1)).T\n",
    "    \n",
    "    z_scores = np.empty(priors.shape[0])\n",
    "    count_matrix = np.empty([2, vocab_size], dtype=np.float32)\n",
    "    count_matrix[0, :] = np.sum(counts_mat[:len(l1), :], axis = 0)\n",
    "    count_matrix[1, :] = np.sum(counts_mat[len(l1):, :], axis = 0)\n",
    "    a0 = np.sum(priors)\n",
    "    n1 = 1.*np.sum(count_matrix[0,:])\n",
    "    n2 = 1.*np.sum(count_matrix[1,:])\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        #compute delta\n",
    "        term1 = np.log((count_matrix[0,i] + priors[i])/(n1 + a0 - count_matrix[0,i] - priors[i]))\n",
    "        term2 = np.log((count_matrix[1,i] + priors[i])/(n2 + a0 - count_matrix[1,i] - priors[i]))        \n",
    "        delta = term1 - term2\n",
    "        #compute variance on delta\n",
    "        var = 1./(count_matrix[0,i] + priors[i]) + 1./(count_matrix[1,i] + priors[i])\n",
    "        #store final score\n",
    "        z_scores[i] = delta/np.sqrt(var)\n",
    "    index_to_term = {v:k for k,v in cv.vocabulary_.items()}\n",
    "    sorted_indices = np.argsort(z_scores)\n",
    "    return_list = []\n",
    "    for i in sorted_indices:\n",
    "        return_list.append((index_to_term[i], z_scores[i]))\n",
    "        \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('portland', 0.7519315076409359), ('basicincome', 0.7529277737864452), ('doctorwho', 0.7550536575541459), ('asoiaf', 0.7717712754236867), ('atheism', 0.7755427008407607), ('blog', 0.790035117701019), ('worldpolitics', 0.801693016767784), ('occupywallstreet', 0.8136298870756509), ('seattle', 0.8260242994134416), ('technology', 0.9088369615983086)]\n",
      "[('overwatch', -1.2927113614115437), ('pokemongo', -1.2785102857437642), ('globaloffensive', -1.2767144216669855), ('leagueoflegends', -1.2089432795482127), ('4chan', -1.1500965027148624), ('theredpill', -1.0171051377670564), ('cringeanarchy', -0.9970115620357181), ('dota2', -0.9467274678179401), ('roastme', -0.9076702534043156), ('2007scape', -0.8671520506447393)]\n"
     ]
    }
   ],
   "source": [
    "comparison_2 = compare2(sfp_subreddits_list, td_subreddits_list)\n",
    "print(comparison_2[-10:])\n",
    "print(comparison_2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Neither as a prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "neither_subreddits_list = []\n",
    "\n",
    "for author in neither_subreddits_by_author.keys():\n",
    "    curr_str = \"\"\n",
    "    \n",
    "    for subreddit in neither_subreddits_by_author[author].keys():\n",
    "        curr_str += (subreddit + \" \")\n",
    "        \n",
    "    neither_subreddits_list.append(curr_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare3(l1, l2, baseline, normalize=False):\n",
    "    cv_baseline = CV(decode_error = 'ignore', min_df = 10, max_df = .5, ngram_range=(1,1),\n",
    "        binary = False, max_features = 15000)\n",
    "    \n",
    "    counts_prior = cv_baseline.fit_transform(baseline).toarray()\n",
    "    priors = np.sum(counts_prior, axis=0)\n",
    "    \n",
    "    cv = CV(decode_error = 'ignore', min_df = 10, max_df = .5, ngram_range=(1,1),\n",
    "        binary = False, max_features = 15000, vocabulary = cv_baseline.vocabulary_)\n",
    "    \n",
    "    counts_mat = cv.fit_transform(l1 + l2).toarray()\n",
    "    vocab_size = len(cv.vocabulary_)\n",
    "    #print(vocab_size)\n",
    "    \n",
    "    #normalization:\n",
    "    if(normalize):\n",
    "        counts_mat = (counts_mat.T/(np.sum(counts_mat, axis=1) + 1)).T\n",
    "    \n",
    "    z_scores = np.empty(priors.shape[0])\n",
    "    count_matrix = np.empty([2, vocab_size], dtype=np.float32)\n",
    "    count_matrix[0, :] = np.sum(counts_mat[:len(l1), :], axis = 0)\n",
    "    count_matrix[1, :] = np.sum(counts_mat[len(l1):, :], axis = 0)\n",
    "    a0 = np.sum(priors)\n",
    "    n1 = 1.*np.sum(count_matrix[0,:])\n",
    "    n2 = 1.*np.sum(count_matrix[1,:])\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        #compute delta\n",
    "        term1 = np.log((count_matrix[0,i] + priors[i])/(n1 + a0 - count_matrix[0,i] - priors[i]))\n",
    "        term2 = np.log((count_matrix[1,i] + priors[i])/(n2 + a0 - count_matrix[1,i] - priors[i]))        \n",
    "        delta = term1 - term2\n",
    "        #compute variance on delta\n",
    "        var = 1./(count_matrix[0,i] + priors[i]) + 1./(count_matrix[1,i] + priors[i])\n",
    "        #store final score\n",
    "        z_scores[i] = delta/np.sqrt(var)\n",
    "    index_to_term = {v:k for k,v in cv.vocabulary_.items()}\n",
    "    sorted_indices = np.argsort(z_scores)\n",
    "    return_list = []\n",
    "    for i in sorted_indices:\n",
    "        return_list.append((index_to_term[i], z_scores[i]))\n",
    "        \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('frugal', 3.2837962683991084), ('mapporn', 3.3301398109000924), ('netflixbestof', 3.3877090531313887), ('politics', 3.6386095624894472), ('thebutton', 3.7618815974563584), ('askscience', 3.77833925460946), ('seattle', 3.822106582141636), ('secretsanta', 4.141789141188557), ('doctorwho', 4.2055421054635715), ('blog', 5.388076676853797)]\n",
      "[('pokemongo', -6.941230985241581), ('overwatch', -6.60472364308496), ('globaloffensive', -5.810640385635588), ('roastme', -5.63380768403611), ('4chan', -5.586541302382348), ('dota2', -4.454882825665547), ('darksouls3', -3.798243471527248), ('mma', -3.753678837700957), ('imgoingtohellforthis', -3.6227099401951213), ('tumblrinaction', -3.5172477773774764)]\n"
     ]
    }
   ],
   "source": [
    "comparison_3 = compare3(sfp_subreddits_list, td_subreddits_list, neither_subreddits_list)\n",
    "print(comparison_3[-10:])\n",
    "print(comparison_3[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('doctorwho', 0.30158956085646954), ('mapporn', 0.3051431127897479), ('seattle', 0.3102731333529693), ('askscience', 0.3287725189128407), ('frugal', 0.3298617991362475), ('thebutton', 0.37994196943699454), ('technology', 0.39849558972243077), ('blog', 0.40533975775212594), ('asoiaf', 0.43100721190189023), ('politics', 0.4484807437591217)]\n",
      "[('4chan', -0.5882915193747954), ('pokemongo', -0.5672638141093601), ('globaloffensive', -0.565718367800873), ('overwatch', -0.5200039468047248), ('pcmasterrace', -0.48442485852551403), ('dota2', -0.4800917097580366), ('leagueoflegends', -0.4525732634171881), ('guns', -0.43718812090301995), ('roastme', -0.40368119202936303), ('tumblrinaction', -0.3957858400269076)]\n"
     ]
    }
   ],
   "source": [
    "#Normalized\n",
    "comparison_4 = compare3(sfp_subreddits_list, td_subreddits_list, neither_subreddits_list, normalize=True)\n",
    "print(comparison_4[-10:])\n",
    "print(comparison_4[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
